<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>代码 on 四序、片羽与摸鱼力学</title>
    <link>https://4xu.xyz/tags/%E4%BB%A3%E7%A0%81/</link>
    <description>Recent content in 代码 on 四序、片羽与摸鱼力学</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Oct 2022 00:19:48 +0800</lastBuildDate><atom:link href="https://4xu.xyz/tags/%E4%BB%A3%E7%A0%81/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>notion花瓣图片更新</title>
      <link>https://4xu.xyz/p/notionupdate/</link>
      <pubDate>Tue, 04 Oct 2022 00:19:48 +0800</pubDate>
      
      <guid>https://4xu.xyz/p/notionupdate/</guid>
      <description>前几个月花瓣网突然抄袭了pinterest更新网站，一直以为影响不大，直到最近浏览notion的花瓣时，才发现了花瓣网也开启了防盗链，于是寻思着把notion上的图片链接改为自己的图床
不过notion api一贯不太好用，只好f12查找需要的block    再保存为json文件
1.先提取block内的图片链接
 代码 import requests import json filepath1 = &amp;quot;k:/office/py/爬虫/花瓣/插画.json&amp;quot; with open(filepath1, &amp;quot;r&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: row_data=json.loads(f.readline()) a=0 while a &amp;lt; 200: i=row_data[&amp;quot;result&amp;quot;][&amp;quot;reducerResults&amp;quot;][&amp;quot;collection_group_results&amp;quot;][&amp;quot;blockIds&amp;quot;][a] block=row_data[&#39;recordMap&#39;][&#39;block&#39;][i][&amp;quot;value&amp;quot;][&amp;quot;content&amp;quot;][0] links=row_data[&#39;recordMap&#39;][&#39;block&#39;][block][&amp;quot;value&amp;quot;][&amp;quot;properties&amp;quot;][&amp;quot;title&amp;quot;][0][0] with open(&amp;quot;k:/office/py/爬虫/花瓣/p-block.csv&amp;quot;, &amp;quot;a&amp;quot;,encoding=&amp;quot;utf-8&amp;quot;, newline=&amp;quot;&amp;quot;)as fo: fo.write(block+&amp;quot;,&amp;quot;+links+&amp;quot;,&amp;quot;+i+&amp;quot;\n&amp;quot;) a+=1   提取出来大致是这样   花瓣链接要整理下   但如果图片超过200个，那么只能先提取block，在根据api提取图片链接
 代码 import requests import json import csv import time class photo(): def add_photo(urls): r = requests.request( &amp;quot;GET&amp;quot;, &amp;quot;https://api.notion.com/v1/blocks/&amp;quot;+urls, headers={&amp;quot;Authorization&amp;quot;: &amp;quot;Bearer &amp;quot; + &amp;quot;自己token&amp;quot;, &amp;quot;Notion-Version&amp;quot;: &amp;quot;2022-02-22&amp;quot;}, ) row_data=json.</description>
    </item>
    
    <item>
      <title>用mt3扒谱</title>
      <link>https://4xu.xyz/p/mt3/</link>
      <pubDate>Sun, 01 May 2022 20:19:48 +0800</pubDate>
      
      <guid>https://4xu.xyz/p/mt3/</guid>
      <description>以前有摸过几个月吉他，现在虽没吉他可弹，但还是喜欢把一些大都是galgamebgm转为吉他谱，不过在扒谱过程中，人菜瘾大的我即使勉强听出音阶，对曲子里的大小调，和弦，复调可所谓是毫无办法。后来发现如果有六线谱，那再转换为吉他谱会便利很多，思考着如果有扒谱软件岂不是更妙，仰仗着ai训练的风靡，逛黄油论坛时找到了mt3模型可用于扒谱
huggingface里面原来调用mt3模型出现了bug，于是稍微改了下，自己部署了一份
部署与稍微改了下    首先打开huggingface官网，创建new space 这时sdk不知选谁好，不过首先排除static，因为调用文件是python格式，后来选2侥幸蒙对  把原来代码复制后，看了下原来的错误日志，有这么一行
WARNING: You are using pip version 22.0.2; however, version 22.0.4 is available. 那我直接在app.py ctrl+v
os.system(&amp;quot;python3 -m pip install --upgrade pip&amp;quot;) 错误的地方还有
 File &amp;quot;app.py&amp;quot;, line 88, in __init__ self.partitioner = t5x.partitioning.ModelBasedPjitPartitioner( AttributeError: module &#39;t5x.partitioning&#39; has no attribute &#39;ModelBasedPjitPartitioner&#39; 度娘回来后，一脸懵逼，于是从原来入手，先是找到app.py 那一行
 self.partitioner = t5x.partitioning.ModelBasedPjitPartitioner( model_parallel_submesh=(1, 1, 1, 1), num_partitions=1) 直接到githubmt3搜索参数，num_partitions，  发觉虽然没有ModelBasedPjitPartitioner，但有PjitPartitioner，名字改完后，顺利部署
 self.partitioner = t5x.partitioning.PjitPartitioner( model_parallel_submesh=(1, 1, 1, 1), num_partitions=1)  食用过程 首先打开这个网址，将音乐格式转为wav  接着打开mt3，将音乐甩上去，大概等上两分钟左右大概率会卡，就会出现乐谱下载  至于打谱软件直接用win10商店中的musescore 3  虽说谱不能百分百准确，但对我来说相对够用</description>
    </item>
    
    <item>
      <title>从pinbox到notion再到notion api</title>
      <link>https://4xu.xyz/p/notion/</link>
      <pubDate>Wed, 26 Jan 2022 20:19:48 +0800</pubDate>
      
      <guid>https://4xu.xyz/p/notion/</guid>
      <description>两年前，为了便于浏览各个网站的收藏文章，于是把它们都整合到pinbox这个软件上，然而整合完自己都忘了。直到最近又有收藏文章的需求，打开它一瞧，除了多了个收费外基本没什么变化，后来发现用邀请码可以创建多级收藏夹，心安理得的继续白嫖，没过多久，估计白嫖多级收藏夹的事被知道了，明明没有达到收藏上限，却再也无法创建新收藏了，虽然一年会员费也不贵，但pinbox略带简陋的界面以及两年来几乎没变化的功能，请容许我拒绝   pinbox界面   开始了寻找替代pinbox软件，起初想自部署笔记软件，但功能太少了，自己又没有服务器，想想还是算了，后来找到了notion，虽然是笔记软件，但完美的契合了要求，notion介绍功能网上很多，懒得说了，感觉这款软件最大的亮点在于白嫖模块化   pinbox笑我贫穷，我笑它不懂死宅    notion api 事情到这里，一般就结束了，无非是换了个软件罢了，但某天躺平在床上刷着豆瓣，偶然发现了notion原来有api的，垂死病中惊坐起，在用过notion功能后，一直想把花瓣网的图片和网易云的歌单导入进去，在看到有api后，开启了折腾之旅
notion api可以结合python使用，python以前从没写过，后来看了下有点像node.js爬虫，仰仗贫瘠的js知识与捉急的智商，抄袭借鉴Notion → 支付宝&amp;amp;微信 → 账单里的代码，头发少了几根后，恼恨愉悦开启爬虫之旅
然而一开始就有问题了，写代码常有的事，输入pip install requests解决
 接着又发现notion api怪得很，用图片链接一定要求有后缀，而花瓣网图片恰恰是链接显示的，还能不能愉快地玩耍  后来思考notion支持导入markdown文件，那先把图片链接保存为md文件，在导入到notion中，再根据api更新里面链接，测试可行后，就先开始爬虫花瓣网
花瓣网 花瓣网虽然有tag功能，但却没有排除关键字搜索，这样找起图片来诸多不易，   花瓣谜一般的搜索功能     notion的筛选   1.图片链接与信息汇总
简单来说就是为每一个图片链接保存为md文件，再把图片中的tag，花瓣链接，源地址保存到汇总.csv中
 代码 import os import requests import re headers={ &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&#39;, &#39;X-Request&#39;: &#39;JSON&#39;, &amp;quot;cookie&amp;quot;:&amp;quot;cookie&amp;quot; } req = requests.</description>
    </item>
    
    <item>
      <title>如何在MARKDOWN中优雅地输入会动的进度条</title>
      <link>https://4xu.xyz/p/progressbar/</link>
      <pubDate>Sat, 09 Oct 2021 15:19:48 +0800</pubDate>
      
      <guid>https://4xu.xyz/p/progressbar/</guid>
      <description>最近想通过进度条记录绘画进度，于是打算抄写一下代码，
打开chrome，由于度娘搜索出来的结果不出所料的垃圾语焉不详，只好到codepenctrl+c借鉴，
迅速找到可借鉴目标，莫得感情的开始ctrl+v写代码  本想着直接抄了事，然而在用主题并不支持jq，又要写头疼的js
在themes\hugo-theme-stack\layouts\partials\article\components\footer.html输入
&amp;lt;script&amp;gt; (function() { var current_progress = document.getElementsByClassName(&amp;quot;current-progress&amp;quot;); var dynamic = document.getElementsByClassName(&amp;quot;dynamic&amp;quot;); for(let r=dynamic.length-1;r&amp;gt;-1;r--) { dynamic[r].style.width=current_progress[r].innerHTML + &amp;quot;%&amp;quot;; dynamic[r].setAttribute(&amp;quot;aria-valuenow&amp;quot;,current_progress[r].innerHTML); dynamic[r].innerHTML=current_progress[r].innerHTML+ &amp;quot;% Complete&amp;quot;; } }()); &amp;lt;/script&amp;gt; 写完之后，并没有出现理想的结果原来css还要支持boostrap,继续搬运  在assets\scss\partials\layout\artcle.scss输入
 @-webkit-keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}@-o-keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}@keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}.progress{height:20px;margin-bottom:20px;overflow:hidden;background-color:#f5f5f5;border-radius:4px;-webkit-box-shadow:inset 0 1px 2px rgba(0,0,0,.1);box-shadow:inset 0 1px 2px rgba(0,0,0,.1)} .progress-bar{float:left;width:0;height:100%;font-size:12px;line-height:20px;color:#fff;text-align:center;background-color:#337ab7;-webkit-box-shadow:inset 0 -1px 0 rgba(0,0,0,.15);box-shadow:inset 0 -1px 0 rgba(0,0,0,.15);-webkit-transition:width .6s ease;-o-transition:width .6s ease;transition:width .6s ease} .progress-bar-striped,.progress-striped .progress-bar{background-image:-webkit-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.</description>
    </item>
    
    <item>
      <title>来点rss订阅</title>
      <link>https://4xu.xyz/p/rss1/</link>
      <pubDate>Wed, 09 Jun 2021 20:19:48 +0800</pubDate>
      
      <guid>https://4xu.xyz/p/rss1/</guid>
      <description>为了减少我的摸鱼，开始思考下时间一般用在哪里，浏览网页应该占了大部分，那
 打开各种网页时间==摸鱼时间
 减少打开网页时间，估计你也想到了，就是rss嘛，不止作为信息聚合，还可以抑制我想打开网页的焦虑，在大学就曾听过了rss，不过缺少可定制rss源，最近发觉有rsshub，妙啊，于是开始做点路由。
简单的做路由有createfeed，不过限制偏多，文章数目也只有5，复杂的直接用rsshub做好了，虽然不太懂，但我从来不生产代码，只是代码搬运工，所以有以下代码
const got = require(&#39;@/utils/got&#39;); const cheerio = require(&#39;cheerio&#39;); module.exports = async (ctx) =&amp;gt; { const response = await got({ method: &#39;get&#39;, url: &#39;https://bgm.tv/timeline&#39;, //做订阅的链接 }); const data = response.data; const $ = cheerio.load(data); const list = $(&#39;.tml_item&#39;); //重复的条目 ctx.state.data = { title: &#39;时间线&#39;, link: &#39;https://bgm.tv/timeline&#39;, item: list &amp;amp;&amp;amp; list .map((index, item) =&amp;gt; { item = $(item); if(item.find(&#39;.info&#39;).text().indexOf(&amp;quot;注册&amp;quot;)==-1 &amp;amp;&amp;amp; item.find(&#39;.info&#39;).text().indexOf(&amp;quot;成为了&amp;quot;) ==-1){ description= `${item.find(&#39;.info&#39;).html()}`; title=item.find(&#39;.info&#39;).text(); link=$(item.find(&#39;.info a&#39;)[2]).</description>
    </item>
    
    <item>
      <title>购书抢券记录</title>
      <link>https://4xu.xyz/p/buybook2021421/</link>
      <pubDate>Fri, 23 Apr 2021 07:19:48 +0800</pubDate>
      
      <guid>https://4xu.xyz/p/buybook2021421/</guid>
      <description>去年觉醒了看书的兴趣后，在今年4.23决定买些实体书存货，不料活动力度实在不足，去年四月初猴当就有三折了，今年直到18号晚才有一次，然而被购物车价格临时变动阴到了，可耻的没有抢到。
虽然猴当有2.5折的卷，然而做法相当618，双11式， 抢券需要一定量赏金，赏金获得需要逛商城，签到，非常繁琐，不想做。
狗东篇
听说狗东也有活动了，去看看有什么券，发现300-50   奇怪的券   第一反应感觉奇怪，去年三三折券一般为600-100，配合100-50活动，可以做到三三折。不过300-50也可以做到三三折，想着没差多少，不错了，没想到的是这里面有个坑等着我。
接着又是极度不友好的零点抢券，心中一晒，尔等怎么抢得过单身多年的手速。零点后，可耻的又没有抢到，这不科学，这一定是脚本，要用魔法打败魔法，虽然我不懂js，但我懂点ahk，于是码了下字
；简单的脚本 $F3:: SetTimer, Closeclick, 100 return $F4:: Pause Closeclick: Click 617,1041 sleep 20 Click 473,654 想着是这下稳了，然而第二天中午依然没抢到券，（⊙ｏ⊙）！！！！ 你们的手速是单身了多少年，100ms的脚本都抢不过。不急，100ms不行，3ms总行了吧
；3ms的脚本 $F3:: SetTimer, Closeclick, 3 return $F4:: Pause Closeclick: Click 617,1041 sleep 1 Click 473,654 晚上，总算抢到了，在狗东挑了几本后，发觉不对劲，以往就只有100-50的活动，现在多了个五折的活动，而且书大多参与的是五折的活动，如果你用抢到券买五折的书，别说三三折了，都快4.5折了。嗯，狗东这操作可以呀，说不定就是想让抢到券自动放弃，虽然狗东你在大气层，但我在电离层，只要我想凑，就可以做到三三折  书也很快到了，一本没塑封差评
 猴当篇
23号，听说猴当有3-1优惠码，继续码脚本
；只会用if 的渣渣 SetTimer, monkeydang, 40 return $F4:: Pause return monkeydang: PixelGetColor, color, 266,620 if (color != &amp;quot;0xFFFFFF&amp;quot;){ Click 213,620,2 sleep 10 send ^c sleep 10 clipboard=%clipboard% if (clipboard = &amp;quot;优惠&amp;quot;){ click 172,616 click 201,620 clipboard=&amp;quot;&amp;quot; }else{ Click 881,881 Pause } }else{ PixelGetColor, nextcolor, 104,598 if(nextcolor = &amp;quot;0x3c46FF&amp;quot;){ Click 898,946 Pause }else{ PixelGetColor, color, 266,620 } } 晚上零点用，发觉图书不适用，这才发现了套路很深很深   假如出现其他活动基本上就用不了   删删改改，总算弄完了，偶然用脚本抢到码，又发现有图书不适用，而且特别鸡贼的是，猴当不告诉你那本不适用，只能自己找不同 后浏览图书才发现下面两张图</description>
    </item>
    
  </channel>
</rss>
